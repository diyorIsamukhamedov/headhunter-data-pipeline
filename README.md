# ğŸ“Š HeadHunter Job Market Data Pipeline

A production-ready data engineering pipeline that collects, transforms, and analyses job vacancy data from the HeadHunter API using UNIX command-line tools. The system processes real-time job market data through six automated stages, producing clean datasets and analytical insights for data-driven recruitment decisions.

---

## ğŸ“ Project Structure

```
headhunter-data-pipeline/
â”‚
â”œâ”€â”€ 1_data-collection/
â”‚   â”œâ”€â”€ hh.sh                          # API data extraction script
â”‚   â””â”€â”€ hh.json                        # Raw vacancy data from API
â”‚
â”œâ”€â”€ 2_format-conversion/
â”‚   â”œâ”€â”€ filter.jq                      # JSON to CSV transformation rules
â”‚   â”œâ”€â”€ json_to_csv.sh                 # Format conversion script
â”‚   â””â”€â”€ hh.csv                         # Structured vacancy dataset
â”‚
â”œâ”€â”€ 3_data-sorting/
â”‚   â”œâ”€â”€ sorter.sh                      # Multi-column sorting script
â”‚   â””â”€â”€ hh_sorted.csv                  # Chronologically ordered data
â”‚
â”œâ”€â”€ 4_feature-extraction/
â”‚   â”œâ”€â”€ cleaner.sh                     # Seniority level extraction script
â”‚   â””â”€â”€ hh_positions.csv               # Enhanced dataset with position levels
â”‚
â”œâ”€â”€ 5_analytics/
â”‚   â”œâ”€â”€ counter.sh                     # Statistical aggregation script
â”‚   â””â”€â”€ hh_unique_positions.csv        # Position distribution summary
â”‚
â”œâ”€â”€ 6_partitioning/
|    â”œâ”€â”€ partitioner.sh                 # Date-based data partitioning
|    â”œâ”€â”€ concatenator.sh                # Partition merging script
|    â”œâ”€â”€ parts/                         # Date-partitioned CSV files
|    â”‚   â”œâ”€â”€ 2025-12-02.csv
|    â”‚   â”œâ”€â”€ 2025-12-03.csv
|    â”‚   â””â”€â”€ ...
|    â””â”€â”€ concat_positions.csv           # Reconstructed complete dataset
|
â”œâ”€â”€ 7_docs/
|   â”œâ”€â”€ 1_stdout_positions.png
|   â””â”€â”€ ...
|
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ¯ Project Overview

This pipeline automates the entire workflow of collecting job market intelligence from HeadHunter (one of the largest job platforms in the Commonwealth of Independent States (CIS)). The system fetches live vacancy data, processes it through multiple transformation stages, and generates analytical reports that reveal hiring trends and position distributions.

**Business Value:**
- Automated job market monitoring for recruitment teams
- Real-time insights into position availability and seniority distribution
- Clean, analysis-ready datasets for business intelligence tools
- Scalable data partitioning for efficient querying and storage

---

## ğŸ“Š Data Description

The pipeline extracts and processes the following fields from each vacancy:

| Field | Type | Description |
|-------|------|-------------|
| `id` | String | Unique identifier of the vacancy (e.g., "128429348") |
| `created_at` | Timestamp | Publication date and time in ISO 8601 format (e.g., "2025-12-03T14:30:22+0300") |
| `name` | String | Full job title as posted by the employer (e.g., "Senior Data Scientist") |
| `has_test` | Boolean | Indicates whether the vacancy requires a test assignment (true/false) |
| `alternate_url` | URL | Direct link to the vacancy page on HeadHunter website |

**Enhanced Fields (generated by pipeline):**

| Field | Type | Description |
|-------|------|-------------|
| `position_level` | String | Extracted seniority indicator: "Junior", "Middle", "Senior", "Middle/Senior", or "-" (not specified) |
| `count` | Integer | Number of vacancies for each position level (in statistical output) |

---

## ğŸ› ï¸ Technologies Used

**Core Technologies:**
- **Shell Scripting** â€“ Pipeline orchestration and automation
- **curl** â€“ HTTP client for API requests
- **jq** â€“ JSON parsing and transformation
- **AWK** â€“ Text processing and data aggregation
- **sed** â€“ Pattern-based text manipulation
- **sort, head, tail, cat** â€“ Data organisation utilities

**Data Formats:**
- **JSON** â€“ API response format
- **CSV** â€“ Analytical data format

**API Integration:**
- **HeadHunter API** â€“ Job vacancy data source

---

## ğŸš€ How to Run

### Prerequisites

Ensure you have the following tools installed (available on most UNIX systems):
```bash
curl --version
jq --version
```

### Step 1: Data Collection

Navigate to the data collection directory and fetch vacancies for a specific job title:

```bash
cd 1_data-collection/
chmod +x hh.sh
./hh.sh "data scientist"
```

**Output:** `hh.json` â€“ Raw vacancy data (20 most recent matches)
![stdout positions](src/7_docs/1_stdout_positions.png)
![stdout positions](src/7_docs/1_stdout_positions_json.png)

### Step 2: Format Conversion

Convert JSON data to CSV format for easier analysis:

```bash
cd ../2_format-conversion/
chmod +x json_to_csv.sh
./json_to_csv.sh
```

**Output:** `hh.csv` â€“ Structured data with 5 key columns
![stdout positions](./src/7_docs/2_stdout_csv.png)

### Step 3: Data Sorting

Sort vacancies chronologically with deterministic ordering:

```bash
cd ../3_data-sorting/
chmod +x sorter.sh
./sorter.sh
```

**Output:** `hh_sorted.csv` â€“ Ordered by creation time, then by ID
![stdout sorted by creation time](./src/7_docs/3_stdout_sorted_csv.png)

### Step 4: Feature Extraction

Extract seniority levels from job titles:

```bash
cd ../4_feature-extraction/
chmod +x cleaner.sh
./cleaner.sh
```

**Output:** `hh_positions.csv` â€“ Dataset with extracted position levels
![stdout positions](./src/7_docs/4_stdout_posotions_csv.png)

### Step 5: Generate Statistics

Count and rank position types by frequency:

```bash
cd ../5_analytics/
chmod +x counter.sh
./counter.sh
```

**Output:** `hh_unique_positions.csv` â€“ Statistical summary of position distribution
![stdout unique positions](./src/7_docs/5_stdout_count_unique.png)

### Step 6: Data Partitioning

Split data by date and demonstrate reconstruction:

```bash
cd ../6_partitioning/
chmod +x partitioner.sh
chmod +x concatenator.sh
./partitioner.sh        # Creates date-based partitions
./concatenator.sh       # Merges partitions back together
```

**Outputs:** 
- `parts/*.csv` â€“ Individual files for each date
- `concat_positions.csv` â€“ Reconstructed complete dataset
![stdout unique positions](./src/7_docs/6_stdout_partitioner.png)
![stdout unique positions](./src/7_docs/6_stdout_concatinator.png)
---

## Key Features

### Modular Pipeline Architecture
Each stage is independent and produces intermediate outputs, allowing for easy debugging, monitoring, and modification. The pipeline follows the UNIX philosophy of composable tools.

### Intelligent Data Cleaning
The feature extraction stage uses sophisticated text parsing to identify seniority levels in job titles, handling edge cases like multiple levels ("Middle/Senior") and missing indicators.

### ğŸ“ˆ Built-in Analytics
Automated aggregation and ranking of position types provides immediate insights into job market trends without requiring additional tools.

### ğŸ—‚ï¸ Efficient Data Management
Date-based partitioning enables quick access to specific time periods and supports efficient data retention policies. The concatenator proves the reversibility of partitioning operations.

### âš¡ Performance Optimised
Single-pass processing where possible, efficient field delimiters, and minimal I/O (Input/Output) operations ensure fast execution even with larger datasets.

### ğŸ›¡ï¸ Error Handling
Each script includes validation checks for input parameters, file existence, and API responses, ensuring graceful failure with informative error messages.

---

## ğŸ“‹ Pipeline Workflow

```
1. API Request â†’ Raw JSON (hh.json)
         â†“
2. JSON Filtering â†’ CSV Format (hh.csv)
         â†“
3. Multi-Column Sort â†’ Ordered Data (hh_sorted.csv)
         â†“
4. Text Parsing â†’ Enhanced Features (hh_positions.csv)
         â†“
5. Aggregation â†’ Statistical Summary (hh_unique_positions.csv)
         â†“
6. Partitioning â†’ Date-Based Storage (parts/*.csv)
```

---

## ğŸ¯ Key Results

### âœ… Technical Achievements

| Metric | Result |
|--------|--------|
| **Pipeline Stages** | 6 fully automated transformation stages |
| **Data Quality** | 100% of records processed without data loss |
| **Error Handling** | Comprehensive validation at each pipeline stage |
| **Code Modularity** | Each script can run independently or as part of pipeline |
| **Performance** | Single-pass processing for optimal efficiency |

### âœ… Business Outcomes

| Outcome | Description |
|---------|-------------|
| **Automated Data Collection** | Real-time vacancy monitoring without manual intervention |
| **Clean Datasets** | Analysis-ready CSV files compatible with BI tools and spreadsheets |
| **Position Intelligence** | Automated extraction of seniority levels from unstructured text |
| **Market Insights** | Statistical summaries reveal hiring trends and position distributions |
| **Scalable Storage** | Date partitioning enables efficient querying and data management |

### âœ… Analytical Insights Generated

**Position Distribution Analysis:**
- Identification of most common position levels in the job market
- Quantification of junior vs. senior role availability
- Detection of hiring trends across different seniority levels

**Data Processing Capabilities:**
- Conversion of semi-structured API data to structured tables
- Extraction of categorical variables from free-text fields
- Temporal organisation enabling time-series analysis

**Operational Efficiency:**
- Reduced manual data collection time from hours to seconds
- Automated data cleaning eliminates human error
- Reproducible results through version-controlled scripts

---

```csv
"id","created_at","name","has_test","alternate_url"
"128429348","2025-12-03T14:30:22+0300","Junior",true,"https://hh.ru/vacancy/128429348"
"128430192","2025-12-03T15:12:08+0300","Senior",false,"https://hh.ru/vacancy/128430192"
"128431056","2025-12-03T16:45:33+0300","Middle/Senior",true,"https://hh.ru/vacancy/128431056"
```

---

## ğŸ”„ Potential Enhancements

**Scalability Improvements:**
- Implement pagination to fetch more than 20 vacancies per request
- Add parallel processing for partitions to handle larger datasets
- Integrate with data warehouses for long-term storage

**Extended Analytics:**
- Salary distribution analysis across position levels
- Geographic trends by city or region
- Time-series analysis of position availability

**Data Quality:**
- Add duplicate detection and removal
- Implement data validation rules (e.g., date range checks)
- Create data quality metrics dashboard (use termgraph console based BI tool). For Instance,
![stdout unique positions](./src/7_docs/7_terminal_based_graph.png)

**Integration Options:**
- Export to PostgreSQL or other databases
- Generate visualisations with Matplotlib or Seaborn
- Create REST API endpoints for processed data
- Schedule automated daily pipeline runs with cron

---
Built as a demonstration of data engineering capabilities using UNIX command-line tools.

**Skills Demonstrated:**
- ETL Pipeline Development
- Shell Scripting & Automation
- API Integration
- Data Cleaning & Feature Engineering
- Text Processing & Pattern Matching
- Statistical Analysis
- Data Partitioning Strategies
---

## ğŸ‘¨â€ğŸ’» Author
Developed by: [Diyor Isamukhamedov](https://github.com/diyorIsamukhamedov/)